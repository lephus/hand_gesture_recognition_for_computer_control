#!/usr/bin/env python3
"""
Dataset Architecture Analyzer
Ph√¢n t√≠ch ki·∫øn tr√∫c v√† ch·∫•t l∆∞·ª£ng dataset HaGRID
Author: L√ä H·ªÆU PH√ö
Main Supervisors: HU·ª≤NH H·ªÆU H∆ØNG
CLASS: K50KHMT - TH·ªä GI√ÅC M√ÅY T√çNH
Date: October 2025
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import cv2
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

class DatasetAnalyzer:
    """Ph√¢n t√≠ch ki·∫øn tr√∫c v√† ch·∫•t l∆∞·ª£ng dataset HaGRID"""
    
    def __init__(self, data_path):
        """
        Initialize dataset analyzer
        
        Args:
            data_path: Path to dataset directory
        """
        self.data_path = Path(data_path)
        self.hagrid_path = self.data_path / "hagrid_30k"
        
        # Analysis results
        self.dataset_info = {}
        self.class_distribution = {}
        self.image_quality_stats = {}
        self.annotations_info = {}
        
        print(f"üìÅ Dataset path: {self.data_path}")
        print(f"üìÅ HaGRID path: {self.hagrid_path}")
        
    def check_dataset_structure(self):
        """Ki·ªÉm tra c·∫•u tr√∫c dataset"""
        print("\nüîç PH√ÇN T√çCH C·∫§U TR√öC DATASET")
        print("=" * 50)
        
        # Ki·ªÉm tra path ch√≠nh
        if not self.data_path.exists():
            print(f"‚ùå Dataset path kh√¥ng t·ªìn t·∫°i: {self.data_path}")
            return False
            
        print(f"‚úÖ Dataset path t·ªìn t·∫°i: {self.data_path}")
        
        # Ki·ªÉm tra HaGRID folder
        if not self.hagrid_path.exists():
            print(f"‚ùå HaGRID folder kh√¥ng t·ªìn t·∫°i: {self.hagrid_path}")
            return False
            
        print(f"‚úÖ HaGRID folder t·ªìn t·∫°i: {self.hagrid_path}")
        
        # Ki·ªÉm tra c√°c gesture folders
        expected_folders = [
            'train_val_call', 'train_val_dislike', 'train_val_fist', 'train_val_four',
            'train_val_like', 'train_val_mute', 'train_val_ok', 'train_val_one',
            'train_val_palm', 'train_val_peace', 'train_val_peace_inverted',
            'train_val_rock', 'train_val_stop', 'train_val_stop_inverted',
            'train_val_three', 'train_val_three2', 'train_val_two_up',
            'train_val_two_up_inverted'
        ]
        
        found_folders = []
        missing_folders = []
        
        for folder in expected_folders:
            folder_path = self.hagrid_path / folder
            if folder_path.exists():
                found_folders.append(folder)
                print(f"   ‚úÖ {folder}")
            else:
                missing_folders.append(folder)
                print(f"   ‚ùå {folder}")
        
        print(f"\nüìä T·ªïng k·∫øt c·∫•u tr√∫c:")
        print(f"   ‚úÖ Found: {len(found_folders)} folders")
        print(f"   ‚ùå Missing: {len(missing_folders)} folders")
        
        if missing_folders:
            print(f"   ‚ö†Ô∏è  Missing folders: {missing_folders}")
        
        self.dataset_info['total_folders'] = len(found_folders)
        self.dataset_info['missing_folders'] = missing_folders
        self.dataset_info['found_folders'] = found_folders
        
        return len(found_folders) > 0
    
    def analyze_class_distribution(self):
        """Ph√¢n t√≠ch ph√¢n ph·ªëi c√°c l·ªõp"""
        print("\nüìä PH√ÇN T√çCH PH√ÇN PH·ªêI C√ÅC L·ªöP")
        print("=" * 50)
        
        if not self.hagrid_path.exists():
            print("‚ùå HaGRID path kh√¥ng t·ªìn t·∫°i!")
            return
        
        # L·∫•y danh s√°ch c√°c gesture classes
        gesture_classes = [
            'call', 'dislike', 'fist', 'four', 'like', 'mute', 'ok', 'one',
            'palm', 'peace', 'peace_inverted', 'rock', 'stop', 'stop_inverted',
            'three', 'three_2', 'two_up', 'two_up_inverted'
        ]
        
        class_counts = {}
        total_images = 0
        
        print("üì∏ S·ªë l∆∞·ª£ng ·∫£nh theo t·ª´ng l·ªõp:")
        for gesture_class in gesture_classes:
            # X√°c ƒë·ªãnh path cho t·ª´ng class
            if gesture_class == 'like':
                class_path = self.hagrid_path / f"train_{gesture_class}"
            else:
                class_path = self.hagrid_path / f"train_val_{gesture_class}"
            
            if class_path.exists():
                # ƒê·∫øm s·ªë ·∫£nh
                image_count = len(list(class_path.glob('*.jpg'))) + len(list(class_path.glob('*.png')))
                class_counts[gesture_class] = image_count
                total_images += image_count
                print(f"   {gesture_class:20s}: {image_count:6d} images")
            else:
                class_counts[gesture_class] = 0
                print(f"   {gesture_class:20s}: {0:6d} images (NOT FOUND)")
        
        print(f"\nüìä T·ªïng k·∫øt ph√¢n ph·ªëi:")
        print(f"   Total images: {total_images:,}")
        print(f"   Number of classes: {len(gesture_classes)}")
        print(f"   Average per class: {total_images/len(gesture_classes):.1f}")
        
        # T√¨m class c√≥ √≠t ·∫£nh nh·∫•t v√† nhi·ªÅu ·∫£nh nh·∫•t
        min_class = min(class_counts.items(), key=lambda x: x[1])
        max_class = max(class_counts.items(), key=lambda x: x[1])
        
        print(f"   Min images: {min_class[0]} ({min_class[1]} images)")
        print(f"   Max images: {max_class[0]} ({max_class[1]} images)")
        
        # T√≠nh ƒë·ªô l·ªách chu·∫©n
        counts = list(class_counts.values())
        std_dev = np.std(counts)
        print(f"   Standard deviation: {std_dev:.1f}")
        
        self.class_distribution = class_counts
        self.dataset_info['total_images'] = total_images
        self.dataset_info['num_classes'] = len(gesture_classes)
        self.dataset_info['avg_per_class'] = total_images/len(gesture_classes)
        self.dataset_info['std_deviation'] = std_dev
        
        return class_counts
    
    def analyze_image_quality(self, sample_size=100):
        """Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng ·∫£nh"""
        print(f"\nüñºÔ∏è  PH√ÇN T√çCH CH·∫§T L∆Ø·ª¢NG ·∫¢NH (Sample: {sample_size})")
        print("=" * 50)
        
        if not self.hagrid_path.exists():
            print("‚ùå HaGRID path kh√¥ng t·ªìn t·∫°i!")
            return
        
        # L·∫•y sample ·∫£nh t·ª´ m·ªói class
        gesture_classes = [
            'call', 'dislike', 'fist', 'four', 'like', 'mute', 'ok', 'one',
            'palm', 'peace', 'peace_inverted', 'rock', 'stop', 'stop_inverted',
            'three', 'three_2', 'two_up', 'two_up_inverted'
        ]
        
        image_sizes = []
        image_formats = []
        corrupted_images = 0
        total_analyzed = 0
        
        for gesture_class in gesture_classes:
            if gesture_class == 'like':
                class_path = self.hagrid_path / f"train_{gesture_class}"
            else:
                class_path = self.hagrid_path / f"train_val_{gesture_class}"
            
            if not class_path.exists():
                continue
                
            # L·∫•y sample ·∫£nh
            image_files = list(class_path.glob('*.jpg')) + list(class_path.glob('*.png'))
            sample_files = image_files[:sample_size]
            
            print(f"   üì∏ Analyzing {gesture_class}...")
            
            for img_path in sample_files:
                try:
                    # Load ·∫£nh
                    img = cv2.imread(str(img_path))
                    if img is None:
                        corrupted_images += 1
                        continue
                    
                    # L·∫•y th√¥ng tin ·∫£nh
                    height, width = img.shape[:2]
                    image_sizes.append((width, height))
                    
                    # L·∫•y format
                    format_type = img_path.suffix.lower()
                    image_formats.append(format_type)
                    
                    total_analyzed += 1
                    
                except Exception as e:
                    corrupted_images += 1
                    print(f"      ‚ùå Error loading {img_path}: {e}")
        
        # Ph√¢n t√≠ch k·∫øt qu·∫£
        if image_sizes:
            widths = [size[0] for size in image_sizes]
            heights = [size[1] for size in image_sizes]
            
            print(f"\nüìä K·∫øt qu·∫£ ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng:")
            print(f"   Total analyzed: {total_analyzed}")
            print(f"   Corrupted images: {corrupted_images}")
            print(f"   Corruption rate: {corrupted_images/total_analyzed*100:.2f}%")
            
            print(f"\nüìê K√≠ch th∆∞·ªõc ·∫£nh:")
            print(f"   Width - Min: {min(widths)}, Max: {max(widths)}, Avg: {np.mean(widths):.1f}")
            print(f"   Height - Min: {min(heights)}, Max: {max(heights)}, Avg: {np.mean(heights):.1f}")
            
            # Ph√¢n t√≠ch format
            format_counts = Counter(image_formats)
            print(f"\nüìÑ Format ph√¢n ph·ªëi:")
            for format_type, count in format_counts.items():
                print(f"   {format_type}: {count} images ({count/total_analyzed*100:.1f}%)")
            
            self.image_quality_stats = {
                'total_analyzed': total_analyzed,
                'corrupted_images': corrupted_images,
                'corruption_rate': corrupted_images/total_analyzed*100,
                'avg_width': np.mean(widths),
                'avg_height': np.mean(heights),
                'min_width': min(widths),
                'max_width': max(widths),
                'min_height': min(heights),
                'max_height': max(heights),
                'format_distribution': dict(format_counts)
            }
        
        return self.image_quality_stats
    
    def analyze_annotations(self):
        """Ph√¢n t√≠ch annotations (n·∫øu c√≥)"""
        print("\nüìã PH√ÇN T√çCH ANNOTATIONS")
        print("=" * 50)
        
        # T√¨m file annotations
        annotation_files = []
        
        # T√¨m file JSON
        json_files = list(self.data_path.glob("*.json"))
        if json_files:
            annotation_files.extend(json_files)
            print(f"üìÑ Found JSON files: {len(json_files)}")
            for f in json_files:
                print(f"   - {f.name}")
        
        # T√¨m file parquet
        parquet_files = list(self.data_path.glob("*.parquet"))
        if parquet_files:
            annotation_files.extend(parquet_files)
            print(f"üìä Found Parquet files: {len(parquet_files)}")
            for f in parquet_files:
                print(f"   - {f.name}")
        
        if not annotation_files:
            print("‚ö†Ô∏è  No annotation files found!")
            return
        
        # Ph√¢n t√≠ch file parquet n·∫øu c√≥
        parquet_file = None
        for f in parquet_files:
            if 'annotations' in f.name.lower():
                parquet_file = f
                break
        
        if parquet_file:
            try:
                print(f"\nüìä Analyzing {parquet_file.name}...")
                df = pd.read_parquet(parquet_file)
                
                print(f"   Rows: {len(df):,}")
                print(f"   Columns: {list(df.columns)}")
                
                # Ph√¢n t√≠ch c·ªôt ch√≠nh
                if 'class' in df.columns:
                    class_counts = df['class'].value_counts()
                    print(f"\nüìä Class distribution in annotations:")
                    for class_name, count in class_counts.items():
                        print(f"   {class_name}: {count}")
                
                self.annotations_info = {
                    'file_path': str(parquet_file),
                    'total_rows': len(df),
                    'columns': list(df.columns),
                    'class_distribution': dict(class_counts) if 'class' in df.columns else {}
                }
                
            except Exception as e:
                print(f"‚ùå Error reading parquet file: {e}")
        
        return self.annotations_info
    
    def create_visualizations(self):
        """T·∫°o visualizations cho ph√¢n t√≠ch"""
        print("\nüìä T·∫†O VISUALIZATIONS")
        print("=" * 50)
        
        # 1. Class distribution bar chart
        if self.class_distribution:
            plt.figure(figsize=(15, 8))
            classes = list(self.class_distribution.keys())
            counts = list(self.class_distribution.values())
            
            bars = plt.bar(classes, counts, color='skyblue', alpha=0.7)
            plt.title('Class Distribution in Dataset', fontsize=16, fontweight='bold')
            plt.xlabel('Gesture Classes', fontsize=12)
            plt.ylabel('Number of Images', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            
            # Th√™m gi√° tr·ªã tr√™n bars
            for bar, count in zip(bars, counts):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, 
                        str(count), ha='center', va='bottom', fontweight='bold')
            
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('class_distribution.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # 2. Image quality pie chart
        if self.image_quality_stats and 'format_distribution' in self.image_quality_stats:
            plt.figure(figsize=(10, 6))
            formats = list(self.image_quality_stats['format_distribution'].keys())
            counts = list(self.image_quality_stats['format_distribution'].values())
            
            plt.pie(counts, labels=formats, autopct='%1.1f%%', startangle=90)
            plt.title('Image Format Distribution', fontsize=16, fontweight='bold')
            plt.axis('equal')
            plt.tight_layout()
            plt.savefig('format_distribution.png', dpi=300, bbox_inches='tight')
            plt.show()
    
    def generate_report(self):
        """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
        print("\nüìã B√ÅO C√ÅO T·ªîNG H·ª¢P")
        print("=" * 60)
        
        print(f"üìÅ Dataset Path: {self.data_path}")
        print(f"üìÅ HaGRID Path: {self.hagrid_path}")
        
        if self.dataset_info:
            print(f"\nüìä Dataset Information:")
            print(f"   Total folders: {self.dataset_info.get('total_folders', 'N/A')}")
            print(f"   Total images: {self.dataset_info.get('total_images', 'N/A'):,}")
            print(f"   Number of classes: {self.dataset_info.get('num_classes', 'N/A')}")
            print(f"   Average per class: {self.dataset_info.get('avg_per_class', 'N/A'):.1f}")
            print(f"   Standard deviation: {self.dataset_info.get('std_deviation', 'N/A'):.1f}")
        
        if self.class_distribution:
            print(f"\nüì∏ Class Distribution:")
            for class_name, count in self.class_distribution.items():
                print(f"   {class_name:20s}: {count:6d} images")
        
        if self.image_quality_stats:
            print(f"\nüñºÔ∏è  Image Quality:")
            print(f"   Total analyzed: {self.image_quality_stats.get('total_analyzed', 'N/A')}")
            print(f"   Corrupted images: {self.image_quality_stats.get('corrupted_images', 'N/A')}")
            print(f"   Corruption rate: {self.image_quality_stats.get('corruption_rate', 'N/A'):.2f}%")
            print(f"   Average size: {self.image_quality_stats.get('avg_width', 'N/A'):.0f}x{self.image_quality_stats.get('avg_height', 'N/A'):.0f}")
        
        if self.annotations_info:
            print(f"\nüìã Annotations:")
            print(f"   File: {self.annotations_info.get('file_path', 'N/A')}")
            print(f"   Total rows: {self.annotations_info.get('total_rows', 'N/A'):,}")
            print(f"   Columns: {self.annotations_info.get('columns', 'N/A')}")
        
        print(f"\n‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!")
    
    def run_full_analysis(self):
        """Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß"""
        print("üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH DATASET")
        print("=" * 60)
        
        # 1. Ki·ªÉm tra c·∫•u tr√∫c
        if not self.check_dataset_structure():
            print("‚ùå Dataset structure check failed!")
            return False
        
        # 2. Ph√¢n t√≠ch ph√¢n ph·ªëi l·ªõp
        self.analyze_class_distribution()
        
        # 3. Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng ·∫£nh
        self.analyze_image_quality()
        
        # 4. Ph√¢n t√≠ch annotations
        self.analyze_annotations()
        
        # 5. T·∫°o visualizations
        self.create_visualizations()
        
        # 6. T·∫°o b√°o c√°o
        self.generate_report()
        
        print("\nüéâ PH√ÇN T√çCH HO√ÄN T·∫§T!")
        return True

def main():
    """Main function"""
    # Set path for Kaggle dataset
    data_path = "/kaggle/input/hagrid-sample/other/default/1/hagrid-sample-30k-384p"
    
    print("üîç DATASET ARCHITECTURE ANALYZER")
    print("=" * 60)
    print(f"üìÅ Data path: {data_path}")
    
    try:
        # Initialize analyzer
        analyzer = DatasetAnalyzer(data_path)
        
        # Run full analysis
        success = analyzer.run_full_analysis()
        
        if success:
            print("\n‚úÖ Analysis completed successfully!")
        else:
            print("\n‚ùå Analysis failed!")
            
    except Exception as e:
        print(f"‚ùå Analysis failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()